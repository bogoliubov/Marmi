\section{Entropia}

Supponiamo di fare un esperimento con $n$ possibili esiti mutualmente esclusivi.
Modelliamo la situazione con uno spazio di probabilità (spazio dei campioni) $(X,\mu)$
e una partizione $X=A_1\sqcup \dots\sqcup A_n$ (il tutto anche mod $0$), con $\mu(A_i)=p_i$. 

Con l'esperimento non conosceremo il preciso elemento $x\in X$ ma solo l'esito $A_i$; vogliamo
trovare un modo di misurare a priori la quantità media di \emph{informazione} su $x$ che si ottiene eseguendo l'esperimento. 
\Eacc naturale supporre che questa informazione media guadagnata, che chiamiamo \emph{entropia}, dipenda solo da $n$ e da $p_1,\dots,p_n$.

Se la situazione è un sistema fisico in evoluzione (ad esempio una scatola divisa in due contenente una particella di gas, di cui
in ogni istante possiamo sapere in quale metà si trova), spesso c'è anche una trasformazione $f:X\to X$
che preserva $\mu$: lavorando a tempo discreto, $f$ dice come si sposta un punto nello spazio delle fasi
all'istante successivo. Eseguendo l'esperimento negli istanti $0,\dots,T$ troveremo ancora più informazione su $x$:
in sostanza conosceremo $\uno_{A_i}\pa{f^k(x)}=\uno_{f^{-k}(A_i)}$, perciò l'entropia sarà quella associata
alla \emph{partizione congiunta} fatta da tutte le possibili intersezioni $A_{i_0}\cap f^{-1}(A_{i_1})\cap \dots\cap f^{-T}(A_{i_T})$.

\subsection{Definizione assiomatica dell'entropia}
Cerchiamo un'espressione esplicita per l'entropia, funzione di $p_1,\dots,p_n$, concentrandoci sulla situazione di una sola misurazione
(cioè senza trasformazioni $f$ e senza i diversi istanti di tempo).
Poiché $\sum_i p_i=1$, cerchiamo per ogni $n\ge 1$ una funzione
\[ H^{(n)}:\Delta^{(n)}\to [0,+\infty), \]
dove $\Delta^{(n)}:=\set{(x_1,\dots,x_n):0\le x_i\le 1,\ \sum_i x_i=1}$ è il simplesso $(n-1)$-dimensionale. Bastano alcuni assiomi di buon senso per determinare completamente $H^{(n)}$:
\begin{lista}
\item simmetria negli argomenti
\item $H^{(n)}(1,0,\dots,0)=0$
\item $H^{(n)}(0,p_2,\dots,p_n)=H^{(n-1)}(p_2,\dots,p_n)$
\item $H^{(n)}(p_1,\dots,p_n)\le H^{(n)}\pa{\frac{1}{n},\dots,\frac{1}{n}}$
\item $H^{(n\ell)}(\pi_{11},\dots,\pi_{1\ell},\dots,\pi_{n1},\dots,\pi_{n\ell})=H^{(n)}(p_1,\dots,p_n)
+\sum_{i=1}^n p_i H^{(\ell)}\pa{\frac{\pi_{i1}}{p_i},\dots,\frac{\pi_{i\ell}}{p_i}}$, dove $p_i:=\sum \pi_{ij}$.
\end{lista}

L'ultimo assioma equivale a chiedere che, data una partizione $(A_{ij})_{1\le i\le n,1\le j\le\ell}$
e posto $B_i:=\sqcup_j A_{ij}$, l'entropia di $(A_{ij})$ sia quella della partizione meno fine $(B_i)$
più la media pesata delle \emph{entropie relative} nei blocchi $B_i$, calcolate con la probabilità condizionata.

\begin{teo}Se le funzioni $H^{(n)}$ soddisfano i suddetti assiomi e sono continue, allora
\[ H^{(n)}(p_1,\dots,p_n)=-c\sum_{i=1}^n p_i\ln p_i \]
per qualche costante $c\ge 0$ indipendente da $n$.
\end{teo}

\begin{proof}Ovviamente $H^{(1)}(1)=0$. Calcoliamo intanto
$K(n):=H^{(n)}\pa{\frac{1}{n},\dots,\frac{1}{n}}$. \\
Grazie a $(3)$ e $(4)$, $K(n)$ è non decrescente. Inoltre $(5)$ dà $K(n\ell)=K(n)+K(\ell)$. \\
Supponendo ora $r,\ell\ge 2$, per ogni $n$ scegliamo $m$ tale che $\ell^m\le r^n\le\ell^{m+1}$.
$K(\cdot)$ e il logaritmo sono entrambi non decrescenti e trasformano prodotti in somme, quindi
\[ \ln(\ell^m)\le\ln(r^n)\le\ln(\ell^{m+1})\implica \frac{m}{n}\le\frac{\ln r}{\ln \ell}\le\frac{m}{n}+\frac{1}{n} \]
\[ K(\ell^m)\le K(r^n)\le K(\ell^{m+1})\implica \frac{m}{n}\le\frac{K(r)}{K(\ell)}\le\frac{m}{n}+\frac{1}{n} \]
e mandando $n\to\infty$ otteniamo $\frac{K(r)}{K(\ell)}=\frac{\ln r}{\ln n}$, da cui $K(n)=c\ln n$ per qualche $c\ge 0$. \\
Osserviamo ora che, grazie a $(3)$, l'assioma $(5)$ vale in questa forma più generale:
\[ H^{\pa{\sum \ell_i}}(\pi_{11},\dots,\pi_{1\ell_1},\dots,\pi_{n1},\dots,\pi_{n\ell_n})
=H^{(n)}(p_1,\dots,p_n)+\sum_{i=1}^n p_i H^{(\ell_i)}\pa{\frac{\pi_{i1}}{p_i},\dots,\frac{\pi_{i\ell_i}}{p_i}} \]
dove come al solito $p_i:=\sum \pi_{ij}$. Quindi se $r_1,\dots,r_n$ sono interi positivi e $s=\sum_i r_i$ otteniamo
\[ H^{(s)}\pa{\frac{1}{s},\dots,\frac{1}{s}}=H^{(n)}\pa{\frac{r_1}{s},\dots,\frac{r_n}{s}}
+\sum_{i=1}^n \frac{r_i}{s}H^{(r_i)}\pa{\frac{1}{r_i},\dots,\frac{1}{r_i}}, \]
ovvero definendo $p_i:=\frac{r_i}{s}$ abbiamo $H^{(n)}(p_1,\dots,p_n)=K(s)-\sum_i p_i K(r_i)=-c\sum_i p_i\ln p_i$.
Per la densità in $\Delta^{(n)}$ dei punti a coordinate razionali e la continuità di $H^{(n)}$ arriviamo alla tesi.
\end{proof}

\begin{oss}Le $H^{(n)}$ definite dalla formula trovata soddisfano effettivamente tutti gli assiomi.
L'unico non ovvio è il $(4)$: la convessità di $x\mapsto x\ln x$ su $[0,\infty)$ dà
$\frac{1}{n}\sum p_i\ln p_i\ge\frac{1}{n}\ln\pa{\frac{1}{n}}$ (perché $\sum p_i=1$), ovvero
$-\sum p_i\ln p_i\le\ln n=K(n)$.
\end{oss}

Scegliamo per convenzione $c=1$.
Data una partizione $\mathcal{P}=\set{A_1,\dots,A_n}$ di $X$, scriveremo anche $H^{(n)}(\mathcal{P})$ o $H(\mathcal{P})$
anziché $H^{(n)}\pa{\mu(A_1),\dots,\mu(A_n)}$.

\subsection{Entropia di Kolmogorov-Sinai}

\begin{defi}Date due partizioni $\mathcal{P}=\set{A_1,\dots,A_n}$ e $\mathcal{Q}=\set{B_1,\dots,B_\ell}$,
definiamo il \emph{raffinamento comune} $\mathcal{P}\vee\mathcal{Q}:=\set{A_i\cap B_j\mid 1\le i\le n,1\le j\le \ell}$.
\end{defi}

Se oltre a una partizione $\mathcal{P}$ abbiamo anche una trasformazione misurabile $f:X\to X$ che preserva $\mu$,
possiamo considerare l'informazione media che si acquisisce dopo $T$ istanti di tempo (rilevando
ad ogni istante $k$ l'insieme $A_i$ in cui finisce $f^k(x)$): questa è $\frac{1}{T}H^{(n^T)}(\mathcal{P}_T)$,
dove $\mathcal{P}_T:=\mathcal{P}\vee\cdots\vee f^{-(T-1)}(\mathcal{P})$ (vista come partizione composta
da $n^T$ insiemi). Osserviamo che abbiamo mediato anche nel tempo, cioè abbiamo diviso per $T$.

\begin{lemma}[Fekete, facoltativo]\label{fekete} Se $a:\N\nonzero\to\R$ è subadditiva, ovvero $a(m+n)\le a(m)+a(n)$, esiste sempre
$\lim_{n\to\infty}\frac{a(n)}{n}$ e coincide con $\inf_n\frac{a(n)}{n}$.
\end{lemma}

\begin{proof}Ovviamente $\liminf_{n\to\infty}\frac{a(n)}{n}\ge\inf_n\frac{a(n)}{n}$. Fissiamo ora $m>0$
e per ogni $n$ facciamo la divisione con resto: $n=km+r$. La subadditività dà
\[ \frac{a(n)}{n}\le\frac{a(km)+a(r)}{km+r}=\frac{k}{km+r}a(m)+\frac{a(r)}{km+r} \]
e mandando $n\to\infty$ (essendo $r$ limitato e così pure $a(r)$) otteniamo $\limsup_{n\to\infty}\le\frac{a(m)}{m}$.
Dato che questo vale per tutti gli $m$ segue la tesi.
\end{proof}

\begin{cor}Esiste sempre $h(f,\mathcal{P}):=\lim_{T\to\infty}\frac{1}{T}H(\mathcal{P}_T)$.
\end{cor}

\begin{proof}Grazie a Fekete, basta mostrare che $H(\mathcal{P}_{m+n})\le H(\mathcal{P}_m)+H(\mathcal{P}_n)$.
Essendo $\mathcal{P}_{m+n}=\mathcal{P}_m\vee f^{-m}(\mathcal{P}_n)$ e $H(\mathcal{P}_n)=H\pa{f^{-m}(\mathcal{P}_n)}$,
basta vedere che $H(\mathcal{P}\vee\mathcal{Q})\le H(\mathcal{P})+H(\mathcal{Q})$ per due generiche
partizioni $\mathcal{P}=\set{A_1,\dots,A_k}$ e $\mathcal{Q}=\set{B_1,\dots,B_\ell}$. \\
Dall'assioma $(5)$ troviamo
\[ H(\mathcal{P}\vee\mathcal{Q})=H(\mathcal{P})+\sum_{i,j}\lambda_i f(t_{ij}) \]
dove $\lambda_i=\mu(A_i)$, $t_{ij}=\frac{\mu(A_i\cap B_j)}{\mu(A_i)}$ e $f(x)=-x\ln x$. \\
Per la concavità di $f$ e il fatto che $\sum_i\lambda_i=1$ otteniamo (Jensen)
\[ \sum_i \lambda_i f(t_{ij})\le f\pa{\sum_i\lambda_i t_{ij}}=f(\mu(B_j))=-\mu(B_j)\ln\mu(B_j), \]
da cui $\sum_{i,j}\lambda_i f(t_{ij})\le -\sum_j \mu(B_j)\ln\mu(B_j)=H(\mathcal{Q})$.
\end{proof}

\begin{oss}La dimostrazione precedente mostra anche che, se $\mathcal{P}$ e $\mathcal{Q}$ sono partizioni indipendenti,
vale $H(\mathcal{P}\vee\mathcal{Q})=H(\mathcal{P})+H(\mathcal{Q})$: infatti in questo caso $t_{ij}=\mu(B_j)$
e quindi $\sum_{i,j}\lambda_i f(t_{ij})=\sum_j f\pa{\mu(B_j)}=H(\mathcal{Q})$.
\end{oss}

\begin{defi}L'\emph{entropia di Kolmogorov-Sinai} del sistema dinamico misurabile $(X,\mathcal{A},\mu,f)$ è
$h_{KS}(f):=\sup_{\mathcal{P}}h(f,\mathcal{P})$, al variare delle partizioni misurabili $\mathcal{P}$.
\end{defi}

\begin{oss}Si vede facilmente che $h_{KS}$ è invariante per isomorfismo di sistemi dinamici misurabili.
\end{oss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Schemi di Bernoulli}


Consideriamo $X:=\set{1,\dots,N}^{\Z}$: questo diventa uno spazio metrico definendo, per $x,y\in X$, $a(x,y):=\min\set{\abs{i}:x_i\neq y_i}$ 
(che vale per convenzione $+\infty$ se $x=y$) e $d(x,y)=2^{-a(x,y)}$. $(X,d)$ è compatto, e lo shift sinistro $\sigma:X\to X$ dato da $\pa{\sigma(x)}_i:=x_{i+1}$ 
è un omeomorfismo di $X$.

Data una misura di probabilità arbitraria su $\set{1,\dots,N}$ che vale $p_i$ su $\set{i}$, possiamo mettere su $X$
la probabilità prodotto $\mu$, che è ovviamente $\sigma$-invariante. 
Chiamiamo \emph{schema di Bernoulli} il sistema dinamico misurabile che abbiamo costruito, e lo indicheremo con $BS(p_1,\dots,p_N)$.

Nel caso di due simboli, che chiamiamo $0$ e $1$ per comodità, abbiamo una mappa 
\[\tilde{\pi}:\set{0,1}^{\Z}\to [0,1]^2  \qquad (x_i)\mapsto (\xi,\eta) \mbox{ con }  \begin{cases} \xi=\sum_{i\ge 0}x_i 2^{-(i+1)} \\ \eta=\sum_{i<0}x_i 2^{-\abs{i}}\end{cases}.\] 
$\tilde{\pi}$ ha inversa definita q.o. e coniuga $\sigma$ nella ``mappa del panettiere'' $f(x,y):=\pa{\pa{2x},\frac{y+\floor{2x}}{2}}$,
che graficamente agisce come

\begin{center}\begin{tikzpicture}[scale=0.70]
\draw (0,0) -- (2,0);
\draw (2,2) -- (2,0);
\draw (2,2) -- (0,2);
\draw (0,2) -- (0,0);
\draw[dashed] (1,0) -- (1,2);
\draw[->] (3,1) -- (4,1);
\draw (5,0.5) -- (7,0.5);
\draw (5,0.5) -- (5,1.5);
\draw (5,1.5) -- (7,1.5);
\draw[dashed] (7,0.5) -- (7,1.5);
\draw (8,0.5) -- (10,0.5);
\draw (10,0.5) -- (10,1.5);
\draw (8,1.5) -- (10,1.5);
\draw[dashed] (8,0.5) -- (8,1.5);
\draw[->] (8.5,2) arc (60:120:2);
\draw[->] (11,1) -- (12,1);
\draw (13,0) -- (15,0);
\draw (13,1) -- (15,1);
\draw (13,2) -- (15,2);
\draw (13,0) -- (13,1);
\draw (15,1) -- (15,2);
\draw[dashed] (13,1) -- (13,2);
\draw[dashed] (15,0) -- (15,1);
\end{tikzpicture}\end{center}


L'entropia di uno schema di Bernoulli si calcola facilmente se diamo per buono questo fatto:

\begin{teo}[Kolmogorov-Sinai]Sia $(X,\mathcal{A},\mu,f)$ con $f$ invertibile e $f^{-1}$ anch'essa misurabile.
Se $\mathcal{P}$ è una partizione \emph{generante}, cioè tale che $\bigvee_{n=-\infty}^\infty T^n(\mathcal{P})=\mathcal{A}$ modulo $0$
(il primo membro è la $\sigma$-algebra generata da $T^n(\mathcal{P})$ per $n\in\Z$), allora $h_{KS}(f)=h(f,\mathcal{P})$.
\end{teo}

Sia $A_k:=\set{x\in X:x_0=k}$ per $k=1,\dots,N$; la partizione $\mathcal{P}:=\set{A_1,\dots,A_N}$ è generante.
Inoltre le partizioni $\sigma^n(\mathcal{P})$ sono tutte indipendenti congiuntamente, quindi per l'ultima osservazione
$H(\mathcal{P}_n)=nH(\mathcal{P})$, per cui $h(\sigma,\mathcal{P})=H(\mathcal{P})$. Dunque, per Kolmogorov-Sinai, l'entropia di 
$BS(p_1,\dots,p_N)$ è $h(\sigma,\mathcal{P})=H(\mathcal{P})=-\sum_{i=1}^N p_i\ln p_i$.

Sappiamo che due sistemi misurabili isomorfi hanno la stessa entropia. Sorprendentemente, per gli schemi di Bernoulli vale anche il viceversa
(che non dimostriamo):

\begin{teo}[Ornstein]Se due schemi di Bernoulli hanno la stessa entropia, sono isomorfi come sistemi dinamici misurabili.
\end{teo}

\subsection{Entropia topologica}

Vogliamo dare una definizione di entropia anche per sistemi dinamici topologici, senza parlare di misura.
Dato $(X,d,f)$ con $X$ compatto e $f:X\to X$ continua, poniamo $d_n(x,y):=\max_{0\le j\le n}d\pa{f^j(x),f^j(y)}$.
Indichiamo con $B(x,r,n)$ le palle rispetto alla distanza $d_n$.

\begin{defi}$S\subseteq X$ è $(n,\epsilon)$-\emph{spanning} se per ogni $x\in X$ c'è un $y\in S$
con $d_n(x,y)<\epsilon$.
\end{defi}

\begin{oss}Per la continuità di $f$, $d_n$ è una distanza equivalente a $d$, cioè
induce la stessa topologia. Dalla compattezza di $X$ segue che esiste sempre un insieme $(n,\epsilon)$-spanning finito.
\end{oss}

\begin{defi}Poniamo $r(n,\epsilon):=\min\set{\abs{S}:S\ (n,\epsilon)\text{-spanning}}$ ($<\infty$
per quanto appena osservato).
\end{defi}

\begin{oss}Se $X$ ha un ricoprimento con $m$ insiemi di diametro $<\epsilon$, $r(n,\epsilon)\le m^{n+1}$.
Infatti, chiamando questi insiemi $A_1,\dots,A_m$, basta prendere un punto da ogni insieme del tipo
$A_{i_0}\cap f^{-1}(A_{i_1})\cap\dots\cap f^{-n}(A_{i_n})$. Ogni $x\in X$ sta in uno di questi $m^{n+1}$ insiemi,
quindi la distanza $d_n$ tra $x$ e il punto corrispondente è $<\epsilon$ per costruzione.
\end{oss}

\begin{defi}L'\emph{entropia topologica} di $(X,d,f)$ è
\[ h_{top}(f):=\lim_{\epsilon\to 0}\limsup_{n\to\infty}\frac{1}{n}\ln r(n,\epsilon). \]
\end{defi}

Il limite esiste perché $\epsilon\mapsto\limsup_{n\to\infty}\frac{1}{n}\ln r(n,\epsilon)$ è decrescente.

\begin{prop}L'entropia topologica ha le seguenti proprietà:
\begin{lista}
\item se $f$ è un'isometria, $h_{top}(f)=0$
\item se $f\in\homeo(X)$, $h_{top}(f^{-1})=h_{top}(f)$
\item $h_{top}(f^m)=mh_{top}(f)$ per $m>0$.
\end{lista}
\end{prop}

\begin{proof}$(1)$: fissato $\epsilon$, scegliamo $S\subseteq X$ finito tale che per ogni $x\in X$
ci sia un $y\in S$ con $d(x,y)<\epsilon$. $S$ è $(n,\epsilon)$-spanning per ogni $n$ in quanto $d_n=d$.
Perciò $\limsup_{n\to\infty}\frac{1}{n}\ln r(n,\epsilon)\le\limsup_{n\to\infty}\frac{\ln\abs{S}}{n}=0$. \\
$(2)$: se $S$ è $(n,\epsilon)$-spanning per $f$, $f^n(S)$ è $(n,\epsilon)$-spanning per $f^{-1}$. \\
$(3)$: chiaramente $r(n,\epsilon,f^m)\le r(mn,\epsilon,f)$ perché un insieme $(mn,\epsilon)$-spanning per $f$
è $(n,\epsilon)$-spanning per $f^m$. Quindi $h_{top}(f^m)\le m h_{top}(f)$.
Per l'altra disuguaglianza: dato $\epsilon$ sia $\delta<\epsilon$ tale che per $d(x,y)<\delta$ si abbia $d\pa{f^j(x),f^j(y)}<\epsilon$,
per ogni $j=0,\dots,m-1$. Ora un insieme $(n,\delta)$-spanning per $f^m$ è $(mn,\epsilon)$-spanning per $f$:
basta scrivere ogni $0\le k\le mn$ come $k=qm+r$ e usare le definizioni.
Quindi $\limsup_{n\to\infty}\frac{1}{n}\ln r(n,\delta,f^m)\ge m\limsup_{n\to\infty}\frac{\ln r(mn,\epsilon,f)}{mn}
=m\limsup_{k\to\infty}\frac{\ln r(k,\epsilon,f)}{k}$.
\end{proof}

\begin{esempio}La rotazione irrazionale $R_\alpha$, essendo un'isometria, ha entropia nulla.
\end{esempio}

\begin{esempio}L'entropia può anche essere infinita. Consideriamo ad esempio
\[ X:=\set{x\in\ell^2(\N):\abs{x_i}\le 2^{-i}} \]
(compatto) con l'endomorfismo $f:X\to X$, $\pa{f(x)}_i:=2x_{i+1}$. \Eacc $r\pa{n,\frac{1}{k}}>k^n$:
infatti $d_n(x,y)\ge\max_{0\le k\le n}\abs{\pa{f^k(x)}_0-\pa{f^k(y)}_0}=\max_{0\le k\le n}2^k\abs{x_k-y_k}$,
quindi, se $S$ è $\pa{n,\frac{1}{k}}$-spanning, scegliendo
\[ x=\sum_{k=0}^n\alpha_k 2^{-k}e_k \]
(con $\abs{\alpha_k}\le 1$) deve esistere un $y\in S$ tale che $\abs{x_k-y_k}<\frac{1}{2^k\cdot k}$,
cioè $\abs{\alpha_k-(2^k y_k)}<\frac{1}{k}$ per $0\le k\le n$.
Ma gli $\alpha_k$ variano in modo arbitrario su $[-1,1]$, quindi
\[ [-1,1]^{n+1}\subseteq \bigcup_{y\in S}B\pa{\pi(y),\frac{1}{k}}, \]
dove $\pi(y)=(y_0,2y_1,\dots,2^n y_n)\in \R^{n+1}$ e le palle sono rispetto alla norma $\norm{\fantasma{a}}_\infty$.
Confrontando le misure, $2^{n+1}\le \abs{S}\cdot\pa{\frac{2}{k}}^{n+1}$ $\implica$ $\abs{S}\ge k^{n+1}$, che è la tesi. 
Dunque $h_{top}(f)=\lim_{k\to\infty}\limsup_{n\to\infty}\frac{1}{n}\ln r\pa{n,\frac{1}{k}}\ge\lim_{k\to\infty}\ln k=\infty$.
\end{esempio}

Vediamo altri modi equivalenti di definire l'entropia topologica.

\begin{defi}$A\subseteq X$ è $(n,\epsilon)$-\emph{separato} se per ogni $x,y\in A$ distinti
vale $d_n(x,y)\ge\epsilon$.
\end{defi}

\begin{prop}Sia $s(n,\epsilon)$ la massima cardinalità di un insieme $(n,\epsilon)$-separato.
Allora $h_{top}(f)=\lim_{\epsilon\to 0}\limsup_{n\to\infty}\frac{1}{n}\ln s(n,\epsilon)$.
\end{prop}

\begin{proof}Sia $A$ un insieme che realizza $s(n,\epsilon)$. $A$ è anche $(n,\epsilon)$-spanning:
ogni $x\in X\setminus A$ dista (nel senso di $d_n$) meno di $\epsilon$ da un elemento di $A$, altrimenti $A$
non sarebbe massimale. Quindi $r(n,\epsilon)\le s(n,\epsilon)$. \\
Inoltre $s(n,2\epsilon)\le r(n,\epsilon)$: siano $A$ è $(n,2\epsilon)$-separato
e $S=\set{x_1,\dots,x_k}$ è $(n,\epsilon)$-spanning; osserviamo che in realtà
$S$ è $(n,\epsilon')$-spanning per qualche $\epsilon'<\epsilon$ (per compattezza di $X$),
quindi $\set{B(x_i,\epsilon',n)\mid i=1,\dots,k}$ è un ricoprimento con diametro massimo (rispetto a $d_n$) $\le 2\epsilon'$,
perciò ogni palla $B(x_i,\epsilon',n)$ contiene al più un elemento di $A$.
Da ciò segue che $\abs{A}\le\abs{S}$. \\
Abbiamo ottenuto $s(n,2\epsilon)\le r(n,\epsilon)\le s(n,\epsilon)$, che dà la tesi.
\end{proof}

\begin{defi}Dati $\alpha,\beta$ ricoprimenti aperti di $X$, definiamo il \emph{ricoprimento congiunto}
$\alpha\vee\beta:=\set{A\cap B\mid A\in\alpha,B\in\beta}$. \\
Dato un ricoprimento aperto $\alpha$, chiamiamo $N(\alpha)$ la cardinalità minima di un sottoricoprimento di $\alpha$
(finita per compattezza).
\end{defi}

\begin{oss}$N(\alpha\vee\beta)\le N(\alpha)N(\beta)$ e $N\pa{f^{-1}(\alpha)}\le N(\alpha)$
(qui $f^{-1}(\alpha)=\set{f^{-1}(A)\mid A\in\alpha}$). \\
Dunque, ponendo temporaneamente $a(n):=\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}$,
\[ a(m+n)\le\ln N\pa{\bigvee_{i=0}^{m-1}f^{-i}(\alpha)}+\ln N\pa{f^{-m}\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}}
\le a(m)+a(n). \]
Perciò, per \hyperref[fekete]{Fekete}, esiste sempre $\lim_{n\to\infty}\frac{1}{n}\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}$.
\end{oss}

Diamo ora una definizione alternativa di entropia topologica, che rende manifesta la somiglianza con l'entropia
di Kolmogorov-Sinai:

\begin{teo}$h_{top}(f)=\sup_\alpha\lim_{n\to\infty}\frac{1}{n}\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}$.
\end{teo}

\begin{proof}[Dimostrazione (facoltativa)]Sia $H$ il secondo membro della tesi. \\
Dato $\alpha=\set{A_i}$ ricoprimento aperto, chiamiamo $\delta>0$ il numero di Lebesgue
associato. Sia ora $S\subseteq X$ $(n-1,\delta)$-spanning; per ogni $x\in S$ e ogni $0\le k\le n-1$ esiste $i_k$ per cui
$B(f^k(x),\delta)\subseteq A_{i_k}$, ovvero
\[ B(x,\delta,n-1)=\bigcap_{k=0}^{n-1} f^{-k}\pa{B\pa{f^k(x),\delta}}\subseteq\bigcap_{k=0}^{n-1} f^{-k}\pa{A_{i_k}}. \]
Dunque $\mathcal{U}:=\set{B(x,\delta,n-1)\mid x\in S}$ è un raffinamento di $\bigvee_{i=0}^{n-1}f^{-i}(\alpha)$,
da cui segue che $\abs{S}\ge N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}$ (per ogni elemento di $\mathcal{U}$
scelgo un elemento di $\bigvee_{i=0}^{n-1}f^{-i}(\alpha)$ che lo include e, siccome $\mathcal{U}$ è anche un ricoprimento,
i sovrainsiemi scelti ricoprono $X$). Infine
\[ h_{top}(f)\ge\limsup_{n\to\infty}\frac{1}{n}\ln r(n-1,\delta)
\ge\limsup_{n\to\infty}\frac{1}{n}\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)} \]
e passando al $\sup$ su $\alpha$ otteniamo $h_{top}(f)\ge H$. \\
Viceversa, fissiamo $\epsilon>0$ e sia $A\subseteq X$ $(n,\epsilon)$-separato di cardinalità massima.
Prendiamo un ricoprimento aperto $\alpha=\set{A_i}$ con $\max_i\diam(A_i)<\epsilon$ e osserviamo
che un elemento di $\bigvee_{k=0}^n f^{-k}(\alpha)$ contiene al più un punto di $A$,
in quanto $x,y\in\bigcap_{k=0}^n f^{-k}\pa{A_{i_k}}$ implica $f^k(x),f^k(y)\in A_{i_k}$ e quindi $d\pa{f^k(x),f^k(y)}<\epsilon$
per ogni $0\le k\le n$, da cui $d_n(x,y)<\epsilon$. \\
Ma allora $s(n,\epsilon)\le N\pa{\bigvee_{k=0}^n f^{-k}(\alpha)}$, da cui
\[ H\ge\lim_{n\to\infty}\frac{1}{n-1}\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}
=\lim_{n\to\infty}\frac{1}{n}\ln N\pa{\bigvee_{i=0}^n f^{-i}(\alpha)}\ge\limsup_{n\to\infty}\frac{1}{n}\ln s(n,\epsilon) \]
e passando al limite per $\epsilon\to 0$ otteniamo $H\ge h_{top}(f)$.
\end{proof}

Concludiamo con il teorema più importante sull'entropia topologica:

\begin{teo}L'entropia topologica non dipende dalla metrica che induce la topologia di $X$.
Dunque è anche invariante per coniugazione.
\end{teo}

\begin{proof}Siano $d$ e $d'$ due metriche che inducono la stessa topologia di $X$ (compatto).
Fissiamo $\epsilon>0$ e poniamo
\[ D_\epsilon:=\set{(x,y)\in X\times X:d(x,y)\ge\epsilon}. \]
Per la continuità di $d:X\times X\to\R$, $D_\epsilon\subseteq X\times X$ è chiuso, quindi compatto (perché $X\times X$ è compatto).
Perciò esiste
\[ \delta'=\delta'(\epsilon):=\min_{(x,y)\in D_\epsilon}d'(x,y)>0 \]
e osserviamo che $d'(x,y)<\delta'\implica (x,y)\nin D_\epsilon\implica d(x,y)<\epsilon$. \\
Quindi, per ogni $x\in X$, $B_{d'}(x,\delta')\subseteq B_d(x,\epsilon)$ e deduciamo
\[ B_{d'_n}(x,\delta')=\bigcap_{k=0}^n f^{-k}\pa{B_{d'}\pa{f^k(x),\delta'}}\subseteq\bigcap_{k=0}^n f^{-k}\pa{B_d\pa{f^k(x),\epsilon}}
=B_{d_n}(x,\epsilon). \]
Ma allora un insieme $(n,\delta')$-spanning (per $d'$) è $(n,\epsilon)$-spanning (per $d$), da cui
$r_{d'}(n,\delta')\ge r_d(n,\epsilon)$ e infine
\[ h_{top}(f,d)=\sup_\epsilon \limsup_{n\to\infty}\frac{1}{n}\ln r_d(n,\epsilon)
\le \sup_\delta'\limsup_{n\to\infty}\frac{1}{n}\ln r_{d'}(n,\delta')=h_{top}(f,d'). \]
Scambiando i ruoli di $d$ e $d'$ otteniamo anche la disuguaglianza opposta.
\end{proof}

\begin{oss}Nonostante l'invarianza per coniugio, abbiamo già visto che in generale $h_{top}(f)\neq h_{top}(f^2)$.
Quindi l'entropia non dà un buon invariante nel caso dei flussi: se $S_t$ è un flusso, posto $f:=S_t$, vorremmo
una quantità $\tilde{h}$ che non cambia nemmeno riparametrizzando il tempo, in particolare
$\tilde{h}(f)=\tilde{h}(S_t)=\tilde{h}(S_{2t})=\tilde{h}(f^2)$.
L'entropia topologica non soddisfa questa proprietà (si dice che non è invariante per \emph{equivalenza orbitale}).
\end{oss}

\subsection{Mappe espansive e altri risultati}

\begin{defi}$f:X\to X$ è \emph{espansiva} se per qualche $\delta>0$ vale l'implicazione
$\forall n\ge 0\ d\pa{f^n(x),f^n(y)}\le\delta\implica x=y$.
\end{defi}

\begin{esempio}$f:\mathbb{T}^1\to\mathbb{T}^1$, $f(x):=px$ è espansiva per $p\ge 2$ intero.
\end{esempio}

Diamo un risultato utile senza dimostrazione:

\begin{teo}Se $f$ è espansiva, allora per ogni $\epsilon>0$ sufficientemente piccolo vale
\[ h_{top}(f)=\lim_{n\to\infty}\frac{1}{n}\ln r(n,\epsilon)=\lim_{n\to\infty}\frac{1}{n}\ln s(n,\epsilon) \]
(l'esistenza dei limiti fa parte dell'enunciato). Inoltre, se $\alpha$ è un ricoprimento aperto con $\max\diam \alpha<\delta$,
\[ h_{top}(f)=\lim_{n\to\infty}\frac{1}{n}\ln N\pa{\bigvee_{i=0}^{n-1}f^{-i}(\alpha)}. \]
\end{teo}

Altri fatti interessanti (sull'entropia di Kolmogorov-Sinai):

\begin{teo}[Shannon-Breiman-McMillan]Siano $(X,\mathcal{A},\mu,f)$ ergodico e $\alpha$ una partizione misurabile finita.
Fissato $x\in X$, indichiamo con $\alpha^n(x)$ l'elemento di $\bigvee_{i=0}^{n-1}f^{-i}(\alpha)$ che contiene $x$
(è ben definito per q.o. $x$). Sia $h_{KS}(f,\alpha):=\lim_{n\to\infty}\frac{1}{n}H\pa{\vee_{i=0}^{n-1}f^{-k}(\alpha)}$
l'entropia relativa alla partizione $\alpha$. Allora, per q.o. $x$,
\[ h_{KS}(f,\alpha)=\lim_{n\to\infty}-\frac{1}{n}\ln \mu\pa{\alpha^n(x)}. \]
\end{teo}

\begin{teo}[Brin-Katok]Siano $(X,d)$ compatto, $f:X\to X$ continua e $\mu$ misura di probabilità invariante ed ergodica.
Allora, per q.o. $x$,
\[ h_{KS}(f)=\sup_\epsilon\limsup_{n\to\infty}-\frac{1}{n}\ln\mu\pa{B(x,\epsilon,n)}. \]
\end{teo}

Quest'ultimo teorema è sorprendente se lo confrontiamo con il teorema di ricorrenza di Poincaré:
se l'entropia di $f$ è positiva, fissato qualsiasi $x_0$ per cui vale la tesi, esistono un $\epsilon>0$ e un $0<\gamma<1$ tali che
$\mu\pa{B(x_0,\epsilon,n)}\le \gamma^n$ definitivamente. \\
Quindi, scelto $\delta\ll\epsilon$, nonostante i punti di $B(x_0,\delta)$ siano ricorrenti, la probabilità che
due punti $x,y\in B(x_0,\delta)$ restino vicini nei primi $n$ istanti scende esponenzialmente con $n$.
Questo suggerisce che ritornino in $B(x_0,\delta)$ in momenti diversi e ``scorrelati''.

Infine entropia topologica e misurabile sono legate da questa relazione:
\begin{teo}Siano $(X,d)$ metrico compatto e $f:X\to X$ continua. Indichiamo con $\mathcal{P}_f(X)$ l'insieme
delle misure di probabilità $f$-invarianti. Vale
\[ h_{top}(f)=\sup_{\mu\in\mathcal{P}_f(X)}h_{KS}(f,\mu). \]
\end{teo}

\subsection{Catene di Markov}

Consideriamo un grafo \emph{diretto} $\Gamma$ su $N$ nodi, cioè in simboli $\Gamma\subseteq\set{1,\dots,N}^2$
(un arco può anche collegare un nodo a se stesso). La sua \emph{matrice di adiacenza} $A\in\R^{N\times N}$
è $a_{ij}:=\begin{cases}1 & \text{se }(i,j)\in\Gamma \\ 0 & \text{altrimenti}\end{cases}$. \\
Sia ora $\Sigma_A:=\set{x\in\set{1,\dots,N}^{\Z}:(x_i,x_{i+1})\in\Gamma}$, ovvero l'insieme dei cammini biinfiniti
su $\Gamma$, e poniamo $\sigma_A:=\restr{\sigma}{\Sigma_A}$ (restrizione dello shift sinistro). \\
Mettiamo su $\set{1,\dots,N}^{\Z}$ la distanza $d(x,y):=2^{-\min\set{\abs{i}:x_i\neq y_i}}$, che lo rende
uno spazio metrico compatto.

\begin{oss}Ovviamente $\sigma_A(\Sigma_A)\subseteq \Sigma_A$ e $\Sigma_A\subseteq\set{1,\dots,N}^{\Z}$ è chiuso,
dunque compatto. $(\Sigma_A,d,\sigma_A)$ è così un sistema dinamico topologico, che si chiama \emph{catena di Markov topologica}.
\end{oss}

Possiamo costruire esplicitamente misure invarianti. Per farlo ci serve il teorema di Perron-Frobenius.

\begin{defi}Data $A\in\R^{N\times N}$ con coefficienti $a_{ij}\ge 0$, diciamo che $A$ è \emph{irriducibile}
se per ogni $i,j$ esiste un $k\ge 0$ tale che $(A^k)_{ij}>0$. Diciamo che $A$ è \emph{primitiva} se
esiste $k\ge 0$ tale che $(A^k)_{ij}>0$ per ogni $i,j$.
\end{defi}

\begin{oss}Se $A$ è irriducibile, $I+A$ è primitiva (lo si vede sviluppando una potenza alta di $I+A$ con il binomio di Newton). \\
Se $A$ è primitiva e $A^k$ ha tutti i coefficienti $>0$, allora anche $A^h$ ha tutti i coefficienti $>0$ per ogni $k\ge h$.
\end{oss}

\begin{oss}$A$ è primitiva se e solo se $\Gamma$ è fortemente connesso, cioè se da ogni nodo si può raggiungere ogni altro nodo.
Infatti $(A^k)_{ij}>0$ se e solo se si può andare dal nodo $i$ al nodo $j$ con un cammino di esattamente $k$ passi.
\end{oss}

\begin{teo}[Perron-Frobenius]Se $A$ è primitiva, esiste un autovalore $\lambda_A>0$ tale che
\begin{itemize}
	\item $\abs{\lambda}<\lambda_A$ per ogni altro autovalore $\lambda\neq\lambda_A$
	\item gli autovettori destri e sinistri di $\lambda_A$ sono $>0$ (cioè hanno tutte le componenti positive) e sono unici
	\item $\lambda_A$ è una radice semplice del polinomio caratteristico $P_A(\lambda)$.
\end{itemize}
\end{teo}

Non dimostriamo Perron-Frobenius; vediamo piuttosto come si deduce il seguente

\begin{cor}Se $A\ge 0$ (coefficienti nonnegativi), esiste $\lambda_A\ge 0$ autovalore tale che
$\abs{\lambda}\le \lambda_A$ per ogni altro autovalore $\lambda$. Inoltre $\lambda_A$ ha un autovettore destro e un autovettore sinistro
con componenti $\ge 0$.
\end{cor}

\begin{proof}Sia $J\in\R^{N\times N}$ la matrice con tutti i coefficienti uguali a $1$. \\
$A_\epsilon:=A+\epsilon J$ è primitiva per ogni $\epsilon>0$. Sia $\rho(A_\epsilon)$ il raggio spettrale di $A_\epsilon$
(il massimo modulo degli autovalori). \\
$\epsilon\mapsto\rho(A_\epsilon)$ è continua, così come i coefficienti del polinomio caratteristico $P_{A_\epsilon}(t)$.
Quindi, da $P_{A_\epsilon}\pa{\rho(A_\epsilon)}=0$ (Perron-Frobenius), mandando $\epsilon\to 0$ otteniamo $P_A{\rho(A)}=0$.
Perciò $\rho(A)$ è autovalore di $A$ ed è il $\lambda_A$ cercato. \\
Infine sia $\epsilon_n\to 0$ una successione qualsiasi. Prendiamo per ogni $n$ un autovettore destro $v_n$
per $A_{\epsilon_n}$, con $\abs{v_n}=1$. A meno di sottosuccessioni, possiamo supporre $v_n\to v$ con $\abs{v}=1$
(compattezza di $S^{N-1}$). Scrivendo $A_{\epsilon_n}v_n=\rho\pa{A_{\epsilon_n}}v_n$ e mandando $n\to\infty$ deduciamo
$Av=\lambda_A v$; infine $v\ge 0$ essendo $v_n>0$. \\
L'esistenza di un autovettore sinistro con componenti $\ge 0$ è analoga.
\end{proof}

Ricordiamo che $v,w\in\R^N$ sono autovettori destro e sinistro di $\lambda_A$ se $Av=\lambda_A v$ e $w^t A=\lambda_A w^t$ rispettivamente.

Si può dimostrare che, se $A$ è primitiva ed è la matrice di adiacenza di un grafo, la catena di Markov
$(\Sigma_A,d,\sigma_A)$ ha entropia topologica $h_{top}(\sigma_A)=\ln \lambda_A$.

Torniamo alla costruzione di una misura invariante: fissato un nodo $i$, assegnamo ad ogni arco $(i,j)\in\Gamma$
una qualsiasi quantità $P_{ij}\ge 0$ in modo che $\sum_j P_{ij}=1$ (poniamo $P_{ij}=0$ se $(i,j)\nin\Gamma$).
$P_{ij}$ rappresenta la probabilità (assegnata da noi) di percorrere l'arco $(i,j)$ partendo da $i$. \\
$P$ è una matrice $N\times N$ e $P\ge 0$. Il suo raggio spettrale è $\rho(P)=1$:
se $Pv=\lambda v$ abbiamo $\abs{\lambda}\abs{v_i}=\abs{\sum_j P_{ij}v_j}\le\sum_j P_{ij}\abs{v_j}\le\max_j\abs{v_j}$
(essendo $\sum_j P_{ij}=1$), da cui $\abs{\lambda}\max_j\abs{v_j}\le\max_j\abs{v_j}$ e quindi $\abs{\lambda}\le 1$;
d'altro canto $(1,\dots,1)$ è autovettore destro di $1$. \\
Quindi per il corollario a Perron-Frobenius esiste un autovettore sinistro
$p=(p_1,\dots,p_N)$ per l'autovalore $1$. \\
Fissato $i_0\in\Z$, assegniamo all'insieme cilindrico
\[ C^{i_0,\cdots, i_0+k}_{j_0,\cdots, j_k}:=\set{x:x_{i_0}=j_0,x_{i_0+1}=j_1,\dots,x_{i_0+k}=j_k}\subseteq\set{1,\dots,N}^{\Z} \]
la misura $p_{j_0}P_{j_0j_1}\cdots P_{j_{k-1}j_k}$
(cioè assegniamo all'evento $\set{x_{i_0}=j_0}$ la probabilità
$p_{j_0}$ e consideriamo il movimento sul grafo come un processo di Markov).
Estendiamo a tutti gli insiemi cilindrici per additività: è facile vedere che la buona definizione della misura sui cilindrici
equivale al fatto che $p$ è autovettore sinistro di $P$
(infatti essendo $C^{i_0\cdots, i_0+k}_{j_0,\cdots, j_k}=\sqcup_{j=1}^N C^{i_0-1,i_0\cdots, i_0+k}_{j,j_0,\cdots, j_k}$
vogliamo che coincidano le quantità  $p_{j_0}P_{j_0 j_1}\cdots P_{j_{k-1}j_k}$ e $\sum_j p_jP_{jj_0}\cdots P_{j_{k-1}j_k}$,
ovvero vogliamo $\sum_j p_j P_{jj_0}=p_{j_0}$). \\
Il teorema di estensione di Kolmogorov permette di estendere il tutto a una misura di probabilità, che è ovviamente $\sigma_A$-invariante
e concentrata su $\Sigma_A$.

Si può dimostrare che l'entropia di Kolmogorov-Sinai di questo sistema misurabile è $-\sum_{i,j}p_iP_{ij}\ln P_{ij}$.

